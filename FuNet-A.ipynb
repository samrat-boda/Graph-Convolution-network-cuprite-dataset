{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import scipy.io as scio \n",
    "import scipy.io as sio  \n",
    "import math\n",
    "import scipy\n",
    "from tensorflow.python.framework import ops \n",
    "from scipy.sparse import csr_matrix \n",
    "from sklearn.model_selection import train_test_split\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(cuprite_data, cnn_data, labels):\n",
    "    # Get the number of spectral bands for the cuprite data\n",
    "    num_spectral_bands_cuprite = cuprite_data.shape[1]\n",
    "    \n",
    "    # Reshape the CNN data to have the same number of spectral bands as the cuprite data\n",
    "    cnn_data = cnn_data.reshape((cnn_data.shape[0], num_spectral_bands_cuprite, 7, 7))\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    cuprite_train, cuprite_test, cnn_train, cnn_test, labels_train, labels_test = train_test_split(\n",
    "        cuprite_data, cnn_data, labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Reshape the CNN data back to its original shape\n",
    "    cnn_train = cnn_train.reshape((cnn_train.shape[0], cnn_train.shape[1]*cnn_train.shape[2]*cnn_train.shape[3]))\n",
    "    cnn_test = cnn_test.reshape((cnn_test.shape[0], cnn_test.shape[1]*cnn_test.shape[2]*cnn_test.shape[3]))\n",
    "    \n",
    "    return cuprite_train, cuprite_test, cnn_train, cnn_test, labels_train, labels_test\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(Labeled_points):\n",
    "    maxVal = np.amax(Labeled_points)\n",
    "    minVal = np.amin(Labeled_points)\n",
    "    Labeled_points_shifted = Labeled_points + abs(minVal)\n",
    "    Labeled_points_norm = Labeled_points_shifted / (abs(minVal) + abs(maxVal))\n",
    "    Labeled_points_norm = Labeled_points_norm + 1e-6\n",
    "\n",
    "    return Labeled_points_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjacency_matrix(ALL_X, k=10):     \n",
    "    N = ALL_X.shape[0]\n",
    "    A = np.zeros((N, N))\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            A[i, j] = cosine_similarity(ALL_X[i], ALL_X[j])\n",
    "    # Convert the graph to a sparse matrix and return it\n",
    "    ALL_L = csr_matrix(A)\n",
    "    return ALL_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches_GCN1(X, X1, Y, L, mini_batch_size, seed):\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation, :]\n",
    "    shuffled_X1 = X1[permutation, :]\n",
    "    shuffled_Y = Y[permutation, :].reshape((m, Y.shape[1]))\n",
    "    shuffled_L1 = L[permutation, :].reshape((L.shape[0], L.shape[1]), order = \"F\")\n",
    "    shuffled_L = shuffled_L1[:, permutation].reshape((L.shape[0], L.shape[1]), order = \"F\")\n",
    "\n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size)\n",
    "    \n",
    "    for k in range(0, num_complete_minibatches):       \n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :]\n",
    "        mini_batch_X1 = shuffled_X1[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :]\n",
    "        mini_batch_L = shuffled_L[k * mini_batch_size : k * mini_batch_size + mini_batch_size, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_X1, mini_batch_Y, mini_batch_L)\n",
    "        mini_batches.append(mini_batch)\n",
    "    mini_batch = (X, X1, Y, L) \n",
    "    mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_x1, n_y):\n",
    "\n",
    "    isTraining = tf.compat.v1.placeholder_with_default(True, shape=())\n",
    "    x_in = tf.compat.v1.placeholder(tf.float32,  [None, n_x], name = \"x_in\")\n",
    "    x_in1 = tf.compat.v1.placeholder(tf.float32,  [None, n_x1], name = \"x_in1\")\n",
    "    y_in = tf.compat.v1.placeholder(tf.float32, [None, n_y], name = \"y_in\")\n",
    "    lap_train = tf.compat.v1.placeholder(tf.float32, [None, None], name = \"lap_train\")\n",
    "    \n",
    "    return x_in, x_in1, y_in, lap_train, isTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "   \n",
    "    tf.compat.v1.set_random_seed(1)\n",
    "\n",
    "    x_w1 = tf.compat.v1.get_variable(\"x_w1\", [224,128], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\", seed = 1))\n",
    "    x_b1 = tf.compat.v1.get_variable(\"x_b1\", [128], initializer = tf.compat.v1.zeros_initializer())\n",
    "    \n",
    "    x_jw1 = tf.compat.v1.get_variable(\"x_jw1\", [128,128], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\", seed = 1))\n",
    "    x_jb1 = tf.compat.v1.get_variable(\"x_jb1\", [128], initializer = tf.compat.v1.zeros_initializer())   \n",
    "    \n",
    "    x_jw2 = tf.compat.v1.get_variable(\"x_jw2\", [128,12], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\", seed = 1))\n",
    "    x_jb2 = tf.compat.v1.get_variable(\"x_jb2\", [12], initializer = tf.compat.v1.zeros_initializer())   \n",
    "    \n",
    "    x_conv_w1 = tf.compat.v1.get_variable(\"x_conv_w1\", [3,3,224,32], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\", seed = 1))\n",
    "    x_conv_b1 = tf.compat.v1.get_variable(\"x_conv_b1\", [32], initializer = tf.compat.v1.zeros_initializer())\n",
    "\n",
    "    x_conv_w2 = tf.compat.v1.get_variable(\"x_conv_w2\", [3,3,32,64], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\", seed = 1))\n",
    "    x_conv_b2 = tf.compat.v1.get_variable(\"x_conv_b2\", [64], initializer = tf.compat.v1.zeros_initializer())\n",
    "\n",
    "    x_conv_w3 = tf.compat.v1.get_variable(\"x_conv_w3\", [1,1,64,128], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\", seed = 1))\n",
    "    x_conv_b3 = tf.compat.v1.get_variable(\"x_conv_b3\", [128], initializer = tf.compat.v1.zeros_initializer())\n",
    "    \n",
    "    parameters = {\"x_w1\": x_w1,\n",
    "                  \"x_b1\": x_b1,\n",
    "                  \"x_jw1\": x_jw1,\n",
    "                  \"x_jb1\": x_jb1,\n",
    "                  \"x_jw2\": x_jw2,\n",
    "                  \"x_jb2\": x_jb2,\n",
    "                  \"x_conv_w1\": x_conv_w1,\n",
    "                  \"x_conv_b1\": x_conv_b1,\n",
    "                  \"x_conv_w2\": x_conv_w2,\n",
    "                  \"x_conv_b2\": x_conv_b2,\n",
    "                  \"x_conv_w3\": x_conv_w3,\n",
    "                  \"x_conv_b3\": x_conv_b3}\n",
    "                  \n",
    "    return parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GCN_layer(x_in, L_, weights):\n",
    "\n",
    "    x_mid = tf.matmul(x_in, weights)\n",
    "    x_out = tf.matmul(L_, x_mid)\n",
    "    \n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mynetwork(x, x1, parameters, Lap, isTraining, momentums = 0.9):\n",
    "\n",
    "    x1 = tf.reshape(x1, [-1, 7, 7, 224], name = \"x1\")\n",
    "    \n",
    "    with tf.compat.v1.name_scope(\"x_layer_1\"):\n",
    "\n",
    "         x_z1_bn = tf.compat.v1.layers.batch_normalization(x, momentum = momentums, training = isTraining)        \n",
    "         x_z1 = GCN_layer(x_z1_bn, Lap, parameters['x_w1']) + parameters['x_b1']\n",
    "         x_z1_bn = tf.compat.v1.layers.batch_normalization(x_z1, momentum = momentums, training = isTraining)\n",
    "         x_a1 = tf.nn.relu(x_z1_bn)\n",
    "\n",
    "         x_conv_z1 = tf.nn.conv2d(x1, filters=parameters['x_conv_w1'], strides=[1, 1, 1, 1], padding='SAME') + parameters['x_conv_b1'] \n",
    "         x_conv_z1_bn = tf.compat.v1.layers.batch_normalization(x_conv_z1, momentum = momentums, training = isTraining)   \n",
    "         x_conv_z1_po = tf.compat.v1.layers.max_pooling2d(x_conv_z1_bn, 2, 2, padding='SAME')\n",
    "         x_conv_a1 = tf.nn.relu(x_conv_z1_po)\n",
    "                 \n",
    "    with tf.compat.v1.name_scope(\"x_layer_2\"):   \n",
    "\n",
    "         x_conv_z2 = tf.nn.conv2d(x_conv_a1, filters=parameters['x_conv_w2'], strides=[1, 1, 1, 1], padding='SAME') + parameters['x_conv_b2'] \n",
    "         x_conv_z2_bn = tf.compat.v1.layers.batch_normalization(x_conv_z2, momentum = momentums, training = isTraining)\n",
    "         x_conv_z2_po = tf.compat.v1.layers.max_pooling2d(x_conv_z2_bn, 2, 2, padding='SAME')\n",
    "         x_conv_a2 = tf.nn.relu(x_conv_z2_po)\n",
    "\n",
    "    with tf.compat.v1.name_scope(\"x_layer_3\"):\n",
    "      \n",
    "         x_conv_z3 = tf.nn.conv2d(x_conv_a2, filters=parameters['x_conv_w3'], strides=[1, 1, 1, 1], padding='SAME') + parameters['x_conv_b3'] \n",
    "         x_conv_z3_bn = tf.compat.v1.layers.batch_normalization(x_conv_z3, momentum = momentums, training = isTraining)\n",
    "         x_conv_z3_po = tf.compat.v1.layers.max_pooling2d(x_conv_z3_bn, 2, 2, padding='SAME')\n",
    "         x_conv_a3 = tf.nn.relu(x_conv_z3_po)\n",
    "         \n",
    "         x_conv_a3_shape = x_conv_a3.get_shape().as_list()\n",
    "         x_conv_z3_2d = tf.reshape(x_conv_a3, [-1, x_conv_a3_shape[1] * x_conv_a3_shape[2] * x_conv_a3_shape[3]])\n",
    "                    \n",
    "         joint_encoder_layer = x_a1 + x_conv_z3_2d\n",
    "         \n",
    "    with tf.compat.v1.name_scope(\"x_joint_layer_1\"):\n",
    "        \n",
    "         x_zj1 = tf.matmul(joint_encoder_layer, parameters['x_jw1']) + parameters['x_jb1']     \n",
    "         x_zj1_bn = tf.compat.v1.layers.batch_normalization(x_zj1, momentum = momentums, training = isTraining)                                 \n",
    "         x_aj1 = tf.nn.relu(x_zj1_bn)   \n",
    "\n",
    "    with tf.compat.v1.name_scope(\"x_layer_4\"):\n",
    "         x_zj2 = tf.matmul(x_aj1, parameters['x_jw2']) + parameters['x_jb2']     \n",
    "                  \n",
    "    l2_loss =  tf.nn.l2_loss(parameters['x_w1']) + tf.nn.l2_loss(parameters['x_jw1']) + tf.nn.l2_loss(parameters['x_jw2'])\\\n",
    "               + tf.nn.l2_loss(parameters['x_conv_w1']) + tf.nn.l2_loss(parameters['x_conv_w2'])  + tf.nn.l2_loss(parameters['x_conv_w3']) \n",
    "                \n",
    "    return x_zj2, l2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mynetwork_optimaization(y_est, y_re, l2_loss, reg, learning_rate, global_step):\n",
    "    \n",
    "    y_re = tf.squeeze(y_re, name = 'y_re')\n",
    "    \n",
    "    with tf.compat.v1.name_scope(\"cost\"):\n",
    "         cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y_est, labels = tf.stop_gradient( y_re))) +  reg * l2_loss\n",
    "         \n",
    "    with tf.compat.v1.name_scope(\"optimization\"):\n",
    "         update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "         optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost,  global_step=global_step)\n",
    "         optimizer = tf.group([optimizer, update_ops])\n",
    "         \n",
    "    return cost, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mynetwork(x_train, x_test, train_x, test_x, y_train, y_test, L_train, L_test, learning_rate_base = 0.001, beta_reg = 0.001, num_epochs = 200, minibatch_size = 32, print_cost = True):\n",
    "    \n",
    "    ops.reset_default_graph()    \n",
    "    tf.compat.v1.set_random_seed(1)                          \n",
    "    seed = 1                                                         \n",
    "    (m, n_x) = x_train.shape\n",
    "    (m, n_y) = y_train.shape\n",
    "    (m, n_x1) = train_x.shape\n",
    "    \n",
    "    costs = []                                        \n",
    "    costs_dev = []\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    \n",
    "    x_in, x_in1, y_in, lap_train, isTraining = create_placeholders(n_x, n_x1, n_y) \n",
    "\n",
    "    parameters = initialize_parameters()\n",
    "    \n",
    "    with tf.compat.v1.name_scope(\"network\"):\n",
    "         x_out, l2_loss = mynetwork(x_in, x_in1, parameters, lap_train, isTraining)\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.compat.v1.train.exponential_decay(learning_rate_base, global_step, 50 * m/minibatch_size, 0.5, staircase = True)\n",
    "    \n",
    "    with tf.compat.v1.name_scope(\"optimization\"):\n",
    "         cost, optimizer = mynetwork_optimaization(x_out, y_in, l2_loss, beta_reg, learning_rate, global_step)\n",
    "\n",
    "    with tf.compat.v1.name_scope(\"metrics\"):\n",
    "         joint_layerT = tf.transpose(x_out)\n",
    "         yT = tf.transpose(y_in)\n",
    "         correct_prediction = tf.equal(tf.argmax(joint_layerT), tf.argmax(yT))\n",
    "         accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "         \n",
    "    init = tf.compat.v1.global_variables_initializer()\n",
    "    \n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        \n",
    "        sess.run(init)\n",
    "\n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs + 1):\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            epoch_acc = 0.\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches_GCN1(x_train, train_x, y_train, L_train, minibatch_size, seed)\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (batch_x, batch_x1, batch_y, batch_l) = minibatch\n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                _, minibatch_cost, minibatch_acc = sess.run([optimizer, cost, accuracy], feed_dict={x_in: batch_x, x_in1: batch_x1, y_in: batch_y, lap_train: batch_l, isTraining: True})\n",
    "           \n",
    "                epoch_cost += minibatch_cost / (num_minibatches+ 1)\n",
    "                epoch_acc += minibatch_acc / (num_minibatches + 1)\n",
    "            \n",
    "            \n",
    "            if print_cost == True and (epoch) % 50 == 0:\n",
    "                features, epoch_cost_dev, epoch_acc_dev = sess.run([x_out, cost, accuracy], feed_dict={x_in: x_test, x_in1: test_x, y_in: y_test, lap_train: L_test, isTraining: False})\n",
    "                print (\"epoch %i: Train_loss: %f, Val_loss: %f, Train_acc: %f, Val_acc: %f\" % (epoch, epoch_cost, epoch_cost_dev, epoch_acc, epoch_acc_dev))\n",
    "       \n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                train_acc.append(epoch_acc)\n",
    "                costs_dev.append(epoch_cost_dev)\n",
    "                val_acc.append(epoch_acc_dev)\n",
    "      \n",
    "        # plot the cost      \n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.plot(np.squeeze(costs_dev))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "        # plot the accuracy \n",
    "        plt.plot(np.squeeze(train_acc))\n",
    "        plt.plot(np.squeeze(val_acc))\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "    \n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "     \n",
    "        return parameters, val_acc, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCN_points = scipy.io.loadmat('Labeled_Cuprite_Data/data_points.mat')['data_points']\n",
    "GCN_points = normalize(GCN_points)\n",
    "\n",
    "CNN_points = scipy.io.loadmat('Labeled_Cuprite_Data/data_points_CNN.mat')['data_points']\n",
    "CNN_points = normalize(CNN_points)\n",
    "\n",
    "Labels = scipy.io.loadmat('Labeled_Cuprite_Data/labels.mat')['labels']\n",
    "Labels = Labels.T\n",
    "num_classes = 12\n",
    "Labels_onehot = tf.keras.utils.to_categorical(Labels, num_classes)\n",
    "\n",
    "# cuprite_train, cuprite_test, cnn_train, cnn_test, labels_train, labels_test\n",
    "Train_X, Test_X, X_train, X_test,TrLabel, TeLabel=split_data(GCN_points, CNN_points, Labels_onehot)\n",
    "\n",
    "\n",
    "print(Train_X.shape)\n",
    "print(Test_X.shape)\n",
    "\n",
    "Train_L = adjacency_matrix(Train_X) \n",
    "Train_L = Train_L.astype(int)\n",
    "Train_L = Train_L.todense() \n",
    "\n",
    "Test_L = adjacency_matrix(Test_X) \n",
    "Test_L = Test_L.astype(int)\n",
    "Test_L = Test_L.todense() \n",
    " \n",
    "\n",
    "\n",
    "parameters, val_acc, features = train_mynetwork(Train_X, Test_X, X_train, X_test, TrLabel, TeLabel, Train_L, Test_L) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
